<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Lv/Lyu Tang</title>

  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Lv/Lyu Tang</name>
              </p>
              <p>My name is Lv Tang. I received my Bachelor of Science degree from the School of Information Science and Technology, Southwest Jiaotong University (SWJTU) in 2018. I obtained my Master‚Äôs degree from the Department of Computer Science, Nanjing University in 2021. I received my Ph.D. degree from the School of Computer Science and Technology, University of Chinese Academy of Sciences (UCAS) in 2025. I will begin my postdoctoral research at the Department of Electrical and Computer Engineering, University of Alberta, in September 2025.
              </p>
              <p style="text-align:center">
                <a href="luckybird1994@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/CV-LvTang.pdf">Resume</a> &nbsp/&nbsp
                <a href="https://twitter.com/luckybird1994">Twitter</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=BSTLuZcAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/luckybird1994">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="data/tanglv-2.jpg"><img style="width:50%;max-width:50%" alt="profile photo" src="data/tanglv-2.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                 I received my Ph.D. degree from the University of Chinese Academy of Sciences (UCAS), where my research focused on large visual models, saliency detection, video compression, camouflaged object detection and image segmentation. Looking ahead, my research interests primarily lie in MLLMs and their applications across various downstream tasks. To date, I have published 10 papers in top-tier conferences and journals, contributing to a total of 29 publications that have accumulated over 890 citations.
              </p>
            </td>
          </tr>
        </tbody></table>


      <!-- News section -->
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <ul>
	      <li><strong>2024-12:</strong> Two papers are accpeted by TOMM 2024 and AAAI2025.</li>
              <li><strong>2024-07:</strong> One paper is accpeted by ACMMM 2024.</li>
              <li><strong>2024-06:</strong> One paper is accpeted by IJCV 2024.</li>
	      <li><strong>2024-03:</strong> One paper is accepted by TOMM 2024.</li>
              <li><strong>2024-02:</strong> One paper is accepted by CVPR 2024.</li>
            </ul>
          </td>
        </tr>
      </tbody></table>
        

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <heading>Selected Publications</heading>

	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/AAAI2025.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ojs.aaai.org/index.php/AAAI/article/view/32934" id="MultiMon">
                <papertitle> Boosting Vision State Space Model with Fractal Scanning
                </papertitle>
              </a>
              <br>
	      <a href="https://scholar.google.com.hk/citations?user=bJmhMKEAAAAJ&hl=zh-CN" target="_blank">Haoke Xiao</a>
	      <strong>Lv Tang*(Corresponding author and Co-first author)</strong>,
	      <a href="https://pengtaojiang.github.io/" target="_blank">Peng-Tao Jiang</a>,
	      <a target="_blank">Hao Zhang</a>,
	      <a target="_blank">Jinwei Chen</a>,
	      <a href="https://libraboli.github.io/" target="_blank">Bo Li</a> 
              <br>
              <em>AAAI 2025 (Oral) </em>

              <br>
              <p></p>
            </td>
          </tr>
		
	  <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/MM2024-CoVP.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2311.11273" id="MultiMon">
                <papertitle> CoVP: Harnessing multimodal large language models for zero-shot camouflaged object detection
                </papertitle>
              </a>
              <br>
	      <strong>Lv Tang</strong>,
	      <a href="https://pengtaojiang.github.io/" target="_blank">Peng-Tao Jiang</a>,
	      <a target="_blank">Zhi-Hao Shen</a>,
	      <a target="_blank">Hao Zhang</a>,
	      <a target="_blank">Jinwei Chen</a>,
	      <a href="https://libraboli.github.io/" target="_blank">Bo Li</a> 
              <br>
              <em>ACMMM 2024 </em>

              <br>
              <p></p>
            </td>
          </tr>


	 <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/IJCV2024-IPSeg.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://link.springer.com/article/10.1007/s11263-024-02185-6" id="MultiMon">
                <papertitle> Towards training-free open-world segmentation via image prompting foundation models
                </papertitle>
              </a>
              <br>
	      <strong>Lv Tang</strong>,
	      <a href="https://pengtaojiang.github.io/" target="_blank">Peng-Tao Jiang</a>,
	      <a href="https://scholar.google.com.hk/citations?user=bJmhMKEAAAAJ&hl=zh-CN" target="_blank">Haoke Xiao</a>,
	      <a href="https://libraboli.github.io/" target="_blank">Bo Li</a> 
              <br>
              <em>IJCV 2024 </em>

              <br>
              <p></p>
            </td>
          </tr>
		
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/CVPR2024-ASAM.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_ASAM_Boosting_Segment_Anything_Model_with_Adversarial_Tuning_CVPR_2024_paper.html" id="MultiMon">
                <papertitle> ASAM: boosting segment anything model with adversarial tuning
                </papertitle>
              </a>
              <br>
	      <a href="https://libraboli.github.io/" target="_blank">Bo Li</a>,
              <a href="https://scholar.google.com.hk/citations?user=bJmhMKEAAAAJ&hl=zh-CN" target="_blank">Haoke Xiao</a>,
              <strong>Lv Tang* (Corresponding author)</strong> 
              <br>
              <em>CVPR 2024 </em>

              <br>
              <p></p>
            </td>
          </tr>

	 <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/TCSVT2024-Matting.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/document/10198480" id="MultiMon">
                <papertitle> From composited to real-world: Transformer-based natural image matting
                </papertitle>
              </a>
              <br>
	      <a target="_blank">Yanfeng Wang</a>,
	      <strong>Lv Tang*</strong>,
	      <a target="_blank">Yi-Jie Zhong</a>,
	      <a href="https://libraboli.github.io/" target="_blank">Bo Li</a> <strong>(Corresponding author)</strong> 
              <br>
              <em>TCSVT 2024 </em>

              <br>
              <p></p>
            </td>
          </tr>

	  <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/TOMM2024-Compression.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://dl.acm.org/doi/10.1145/3661311" id="MultiMon">
                <papertitle> High Efficiency Deep-learning Based Video Compression
                </papertitle>
              </a>
              <br>
	      <strong>Lv Tang</strong>,
	      <a href="https://scholar.google.com/citations?hl=en&user=KQB-cKAAAAAJ" target="_blank">Xinfeng Zhang</a>
              <br>
              <em>TOMM 2024 </em>
              <br>
              <p></p>
            </td>
          </tr>

	  <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/ICCV2023-MVC.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Tang_Scene_Matters_Model-based_Deep_Video_Compression_ICCV_2023_paper.html" id="MultiMon">
                <papertitle> Scene Matters: Model-based Deep Video Compression
                </papertitle>
              </a>
              <br>
	      <strong>Lv Tang</strong>,
	      <a href="https://scholar.google.com/citations?hl=en&user=KQB-cKAAAAAJ" target="_blank">Xinfeng Zhang</a>,
	      <a target="_blank">Gai Zhang</a>,
	      <a target="_blank">Xiaoqi Ma</a>
              <br>
              <em>ICCV 2023 </em>
              <br>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/TIP2022-CoSOD.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/document/9918626" id="MultiMon">
                <papertitle> Toward stable co-saliency detection and object co-segmentation
                </papertitle>
              </a>
              <br>
	      <a href="https://libraboli.github.io/" target="_blank">Bo Li</a>,
	      <strong>Lv Tang*</strong>,
	      <a target="_blank">Senyun Kuang</a>,
	      <a href="https://palm.seu.edu.cn/smf/index.html" target="_blank">Mofei Song</a>,
	      <a target="_blank">Shouhong Ding</a> <strong>(Corresponding author)</strong> 
              <br>
              <em>TIP 2022 </em>
              <br>
              <p></p>
            </td>
          </tr>

	   <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/TCSVT2022-CoSOD.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/document/9709799" id="MultiMon">
                <papertitle> Re-thinking the relations in co-saliency detection
                </papertitle>
              </a>
              <br>
	      <strong>Lv Tang</strong>,
	      <a href="https://libraboli.github.io/" target="_blank">Bo Li</a>,
	      <a target="_blank">Senyun Kuang</a>,
	      <a href="https://palm.seu.edu.cn/smf/index.html" target="_blank">Mofei Song</a>,
	      <a target="_blank">Shouhong Ding</a>
              <br>
              <em>TCSVT 2022 </em>
              <br>
              <p></p>
            </td>
          </tr>

	  <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/CVPR2022-COD.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhong_Detecting_Camouflaged_Object_in_Frequency_Domain_CVPR_2022_paper.html" id="MultiMon">
                <papertitle> Detecting camouflaged object in frequency domain
                </papertitle>
              </a>
              <br>
	      <a target="_blank">Yi-Jie Zhong</a>,
	      <a href="https://libraboli.github.io/" target="_blank">Bo Li</a>,
	      <strong>Lv Tang*#</strong>,
	      <a target="_blank">Senyun Kuang</a>,
	      <a target="_blank">Shuang Wu</a>,
	      <a target="_blank">Shouhong Ding</a> <strong>(Corresponding and Co-first author)</strong>
              <br>
              <em>CVPR 2022 </em>
              <br>
              <p></p>
            </td>
          </tr>

	  <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/ICCV2021-HRSOD.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Tang_Disentangled_High_Quality_Salient_Object_Detection_ICCV_2021_paper.html" id="MultiMon">
                <papertitle> Disentangled high quality salient object detection
                </papertitle>
              </a>
              <br>
	      <strong>Lv Tang</strong>,
	      <a href="https://libraboli.github.io/" target="_blank">Bo Li</a>,
	      <a target="_blank">Yi-Jie Zhong</a>,
	      <a target="_blank">Senyun Kuang</a>,
	      <a target="_blank">Shouhong Ding</a>,
	      <a href="https://palm.seu.edu.cn/smf/index.html" target="_blank">Mofei Song</a>
              <br>
              <em>ICCV 2021 </em>
              <br>
              <p></p>
            </td>
          </tr>

        <style>
          .sub-table {
              width: 0;
              height: 0;
              overflow: hidden;
              position: absolute;
              left: -9999px;
              opacity: 0;
              visibility: hidden;
          }
      </style>
      
      <table class="sub-table" align="center">
          <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=y35-AqSkeLIkce_C13W-97DGULFZQWj5YJB3rNARabY&cl=ffffff&w=a"></script>
      </table>
      
        </tbody></table>




      </td>
    </tr>
  </table>
</body>

</html>
