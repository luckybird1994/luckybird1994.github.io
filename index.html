<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Lv Tang</title>

  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Lv Tang</name>
              </p>
              <p>My name is Lv Tang. I received the BSc degree from the School of Information Science and Technology, Southwest Jiaotong University (SWJTU), 2018. I received the Master degree from the Department of Computer Science, Nanjing University, 2021. I am currently a PhD student in University of Chinese Academy of Sciences (UCAS).
              </p>
              <p style="text-align:center">
                <a href="luckybird1994@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/CV-LvTang.pdf">Resume</a> &nbsp/&nbsp
                <a href="https://twitter.com/luckybird1994">Twitter</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=BSTLuZcAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/luckybird1994">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="data/tanglv-2.jpg"><img style="width:50%;max-width:50%" alt="profile photo" src="data/tanglv-2.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                 I am currently a third-year Ph.D. student at UCAS. My research interests mainly include large visual models, saliency detection, video compression, camouflaged object detection, and image segmentation. My current research interests primarily focus on enhancing the performance of LVM/MLLM in a resource-friendly manner and exploring the performance limits of LVM/MLLM in various downstream tasks. In conclusion, I have published 10 papers in top-tier conferences and journals, contributing to a total of 21 publications that have amassed over 524 citations.
              </p>
            </td>
          </tr>
        </tbody></table>


      <!-- News section -->
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <ul>
	      <li><strong>2024-12:</strong> Two papers are accpeted by TOMM 2024 and AAAI2025.</li>
              <li><strong>2024-07:</strong> One paper is accpeted by ACMMM 2024.</li>
              <li><strong>2024-06:</strong> One paper is accpeted by IJCV 2024.</li>
	      <li><strong>2024-03:</strong> One paper is accepted by TOMM 2024.</li>
              <li><strong>2024-02:</strong> One paper is accepted by CVPR 2024.</li>
            </ul>
          </td>
        </tr>
      </tbody></table>
        

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <heading>Selected Publications</heading>
		
	  <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/MM2024-CoVP.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2311.11273" id="MultiMon">
                <papertitle> CoVP: Harnessing multimodal large language models for zero-shot camouflaged object detection
                </papertitle>
              </a>
              <br>
	      <strong>Lv Tang</strong>,
	      <a href="https://pengtaojiang.github.io/" target="_blank">Peng-Tao Jiang</a>,
	      <a target="_blank">Zhi-Hao Shen</a>,
	      <a target="_blank">Hao Zhang</a>,
	      <a target="_blank">Jinwei Chen</a>,
	      <a href="https://libraboli.github.io/" target="_blank">Bo Li</a> 
              <br>
              <em>ACMMM 2024 </em>

              <br>
              <p></p>
            </td>
          </tr>


	 <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/IJCV2024-IPSeg.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://link.springer.com/article/10.1007/s11263-024-02185-6" id="MultiMon">
                <papertitle> Towards training-free open-world segmentation via image prompting foundation models
                </papertitle>
              </a>
              <br>
	      <strong>Lv Tang</strong>,
	      <a href="https://pengtaojiang.github.io/" target="_blank">Peng-Tao Jiang</a>,
	      <a href="https://scholar.google.com.hk/citations?user=bJmhMKEAAAAJ&hl=zh-CN" target="_blank">Haoke Xiao</a>,
	      <a href="https://libraboli.github.io/" target="_blank">Bo Li</a> 
              <br>
              <em>IJCV 2024 </em>

              <br>
              <p></p>
            </td>
          </tr>
		
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/CVPR2024-ASAM.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_ASAM_Boosting_Segment_Anything_Model_with_Adversarial_Tuning_CVPR_2024_paper.html" id="MultiMon">
                <papertitle> ASAM: boosting segment anything model with adversarial tuning
                </papertitle>
              </a>
              <br>
	      <a href="https://libraboli.github.io/" target="_blank">Bo Li</a>,
              <a href="https://scholar.google.com.hk/citations?user=bJmhMKEAAAAJ&hl=zh-CN" target="_blank">Haoke Xiao</a>,
              <strong>Lv Tang* (Corresponding author)</strong> 
              <br>
              <em>CVPR 2024 </em>

              <br>
              <p></p>
            </td>
          </tr>

	 <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/TCSVT2024-Matting.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/document/10198480" id="MultiMon">
                <papertitle> From composited to real-world: Transformer-based natural image matting
                </papertitle>
              </a>
              <br>
	      <a target="_blank">Yanfeng Wang</a>,
	      <strong>Lv Tang*</strong>,
	      <a target="_blank">Yi-Jie Zhong</a>,
	      <a href="https://libraboli.github.io/" target="_blank">Bo Li</a> <strong>(Corresponding author)</strong> 
              <br>
              <em>TCSVT 2024 </em>

              <br>
              <p></p>
            </td>
          </tr>

	  <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/TOMM2024-Compression.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://dl.acm.org/doi/10.1145/3661311" id="MultiMon">
                <papertitle> High Efficiency Deep-learning Based Video Compression
                </papertitle>
              </a>
              <br>
	      <strong>Lv Tang</strong>,
	      <a href="https://scholar.google.com/citations?hl=en&user=KQB-cKAAAAAJ" target="_blank">Xinfeng Zhang</a>
              <br>
              <em>TOMM 2024 </em>
              <br>
              <p></p>
            </td>
          </tr>

	  <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/ICCV2023-MVC.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Tang_Scene_Matters_Model-based_Deep_Video_Compression_ICCV_2023_paper.html" id="MultiMon">
                <papertitle> Scene Matters: Model-based Deep Video Compression
                </papertitle>
              </a>
              <br>
	      <strong>Lv Tang</strong>,
	      <a href="https://scholar.google.com/citations?hl=en&user=KQB-cKAAAAAJ" target="_blank">Xinfeng Zhang</a>,
	      <a target="_blank">Gai Zhang</a>,
	      <a target="_blank">Xiaoqi Ma</a>
              <br>
              <em>ICCV 2023 </em>
              <br>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/TIP2022-CoSOD.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/document/9918626" id="MultiMon">
                <papertitle> Toward stable co-saliency detection and object co-segmentation
                </papertitle>
              </a>
              <br>
	      <a href="https://libraboli.github.io/" target="_blank">Bo Li</a>,
	      <strong>Lv Tang*</strong>,
	      <a target="_blank">Senyun Kuang</a>,
	      <a href="https://palm.seu.edu.cn/smf/index.html" target="_blank">Mofei Song</a>,
	      <a target="_blank">Shouhong Ding</a> <strong>(Corresponding author)</strong> 
              <br>
              <em>TIP 2022 </em>
              <br>
              <p></p>
            </td>
          </tr>

	   <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/TCSVT2022-CoSOD.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/document/9709799" id="MultiMon">
                <papertitle> Re-thinking the relations in co-saliency detection
                </papertitle>
              </a>
              <br>
	      <strong>Lv Tang</strong>,
	      <a href="https://libraboli.github.io/" target="_blank">Bo Li</a>,
	      <a target="_blank">Senyun Kuang</a>,
	      <a href="https://palm.seu.edu.cn/smf/index.html" target="_blank">Mofei Song</a>,
	      <a target="_blank">Shouhong Ding</a>
              <br>
              <em>TCSVT 2022 </em>
              <br>
              <p></p>
            </td>
          </tr>

	  <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/CVPR2022-COD.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhong_Detecting_Camouflaged_Object_in_Frequency_Domain_CVPR_2022_paper.html" id="MultiMon">
                <papertitle> Detecting camouflaged object in frequency domain
                </papertitle>
              </a>
              <br>
	      <a target="_blank">Yi-Jie Zhong</a>,
	      <a href="https://libraboli.github.io/" target="_blank">Bo Li</a>,
	      <strong>Lv Tang*#</strong>,
	      <a target="_blank">Senyun Kuang</a>,
	      <a target="_blank">Shuang Wu</a>,
	      <a target="_blank">Shouhong Ding</a> <strong>(Corresponding and Co-first author)</strong>
              <br>
              <em>CVPR 2022 </em>
              <br>
              <p></p>
            </td>
          </tr>

	  <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/ICCV2021-HRSOD.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Tang_Disentangled_High_Quality_Salient_Object_Detection_ICCV_2021_paper.html" id="MultiMon">
                <papertitle> Disentangled high quality salient object detection
                </papertitle>
              </a>
              <br>
	      <strong>Lv Tang</strong>,
	      <a href="https://libraboli.github.io/" target="_blank">Bo Li</a>,
	      <a target="_blank">Yi-Jie Zhong</a>,
	      <a target="_blank">Senyun Kuang</a>,
	      <a target="_blank">Shouhong Ding</a>,
	      <a href="https://palm.seu.edu.cn/smf/index.html" target="_blank">Mofei Song</a>
              <br>
              <em>ICCV 2021 </em>
              <br>
              <p></p>
            </td>
          </tr>

        <style>
          .sub-table {
              width: 0;
              height: 0;
              overflow: hidden;
              position: absolute;
              left: -9999px;
              opacity: 0;
              visibility: hidden;
          }
      </style>
      
      <table class="sub-table" align="center">
          <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=y35-AqSkeLIkce_C13W-97DGULFZQWj5YJB3rNARabY&cl=ffffff&w=a"></script>
      </table>
      
        </tbody></table>




      </td>
    </tr>
  </table>
</body>

</html>
